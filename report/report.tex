\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{linguex}
\usepackage{booktabs} % For professional quality lines
\usepackage{multirow} % Optional, for grouping rows

\graphicspath{{..}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Part-of-Speech Tagging in English using Contextual Word Embeddings}

\author{\IEEEauthorblockN{Juhani Dickinson}
\IEEEauthorblockA{\texttt{jdickin9@uwo.ca}}
\and
\IEEEauthorblockN{Paul Moore}
\IEEEauthorblockA{\texttt{email address or ORCID}}
}

\maketitle

\begin{abstract}
ABSTRACT
\end{abstract}

\begin{IEEEkeywords}
pos tagging, part of speech, nlp, word embedding, contextual word embedding
\end{IEEEkeywords}

\section{Introduction}
\subsection{Parts of Speech}
In a sentence, each word has a syntactic role to play. In the sentence ``The brown dog runs quickly towards food.'', ``dog'' denotes a thing, ``brown'' modifies or describes ``dog'', ``runs'' denotes an action, and so on. These syntactic roles are commonly referred to as ``parts of speech'', and are mapped to words using part-of-speech tags.
However, this is not a one-to-one mapping. Many words have multiple meanings, and in many cases, these different meanings play different syntactic roles, mapping them to different parts of speech. For example, the word ``orange'', spelled and pronounced identically, can be either a noun or an adjective, as seen in sentences (\ref{orange_n}) and (\ref{orange_adj}).
\begin{align}
	\text{``I often eat an }&\text{\underline{orange} after working out.'' (Noun)}\label{orange_n}\\
	\text{``The }&\text{\underline{orange} car has arrived.'' (Adjective)}\label{orange_adj}
\end{align}

Figuring out what part of speech a word has is not an unsolvable task, however. Where a word is in the sentence and what words surround it play a major role in disambiguating between syntactic roles.
In (\ref{orange_n}), the word after ``orange'' is ``after'', which is an adverb. ``Orange'' is either an adjective or a noun, and since the following word is an adverb, it cannot be an adjective. In (\ref{orange_adj}), the words surrounding ``orange'' are ``the'' and ``car'', which are a determiner and a noun respectively. In this position, ``orange'' describes ``car'', making it an adjective.

\subsection{Part-of-Speech (POS) Tagging}
The task of POS tagging (often simply called tagging) is as follows: given a sentence, label each word in the sentence with the POS tag that describes its syntactic role, seen below for sentence \ref{pos_tag_example}.
\addtocounter{ExNo}{\value{equation}}\stepcounter{equation} %matching the linguistic example counter to align counter
%This does need 2 blank lines under it to start a new paragraph
\exg.\label{pos_tag_example} Can you \underline{spot} the large \underline{spot} on your nose ?\\
	{} AUX D VB D ADJ N PP D N .\\


What makes the task of tagging challenging is the contextual requirement: without it, both instances of ``spot'' in sentence \ref{pos_tag_example} would be tagged as either VB or N. Because of this, non-naive tagger implementations leverage some level of context to perform their task. Simple implementations like $n$-gram taggers assign tags to words based on both the word itself and the tags of the preceding $n$ words.
%something about n-gram taggers?

\subsection{Word Embeddings}
A common issue with simple tagging implementations is resistance to parallelisation. Any operation below the level of sentence must be linear, as the tag of the current word depends on the tag(s) of the previous words.
To get around the linear-time requirement, some taggers use word embeddings, which represent words as vectors. This embedding function takes into account the surrounding words, so that the meaning and context of each word is stored in the embedding.
Word embeddings are classifier models usually trained using one of two methods: predict target word from context (Continuous Bag of Words, CBOW) and predict context from target word (Skipgram).
In CBOW, the words surrounding the target word are mapped to one-hot vectors, which are each then projected down to a dense vector by the classifier model and averaged together. This averaged vector is then used to predict the target word.
In Skipgram, the target word is mapped to a one-hot vector and then projected down to a dense vector by the classifier. This is done for multiple target words, the dense vectors of which are averaged together and used to predict the context in which the target words appear.
Using embeddings, tagging can be parallelised at the level of individual words.

\subsubsection{Static Word Embeddings}
%source: https://courses.grainger.illinois.edu/cs546/sp2020/Slides/Lecture04.pdf
%source: https://dev.to/ahikmah/understanding-the-evolution-of-word-representation-static-vs-dynamic-embeddings-5331
While word embeddings take context into account by design, static word embeddings take the naive approach to context. These mapping functions collect all contexts for a given word and uses them to generate a single embedding per word. Often, this approach fails to take into account that words can have different meanings (``river \underline{bank}'' vs. ``\underline{bank} account'') or different syntactic roles. This means that a singular word embedding must hold information on every possible context for any given word, which causes problems when the different meanings of a word are not syntactically or semantically close to one another.

\subsubsection{Contextual Word Embeddings}
%source: https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec3-contextualized-word-embeddings.pdf
Contextual word embeddings solve the syntactic and semantic problems that static word embeddings have by moving from word embeddings to word token embeddings. Now, words with multiple meanings or syntactic roles are split into multiple word tokens, one for each use.
Contextual embeddings map these tokens to vectors, preserving specific meaning. With contextual embeddings, a word like ``spot'' is first split into two tokens, such as ``spot--noun'' and ``spot--verb'', and these tokens are embedded separately. While contextual embedding systems are more computationally demanding to train and use, their context-aware nature often leads to increased performance in context-sensitive tasks such as POS tagging.

\section{Motivations}\label{sec_motivations}
%POS tagging benefit to NLP: TTS, word-sense disambiguation, word order, <--more?-->
%source: http://103.203.175.90:81/fdScript/RootOfEBooks/E%20Book%20collection%20-%202024%20-%20D/CSE%20%20IT%20AIDS%20ML/NATURAL%20LANGUAGE%20PROCESSING%20IN%20ARTIFICIAL%20INTELLIGENCE.pdf#page=150
POS tags are useful and sometimes key pieces of information in many natural language processing (NLP) tasks such as text-to-speech (TTS), word-sense disambiguation, marking word order, and named entity recognition (<ref>).
In TTS applications, the pronunciation of a word can change depending on what its syntactic role is or sometimes even what tense it is. The word ``resume'' is pronounced differently based on whether it is meant as ``continue doing'' or as ``a document detailing professional experience'', which can be easily disambiguated based on whether it acts as a verb or a noun.
This disambiguation-by-tag is also used in word-sense disambiguation, like in sentence \ref{pos_tag_example}. In a search-engine setting, understanding whether ``spot'' refers to an activity or a visual marker allows for more accurate search results.

\subsection{Benefits for NLP in Low-Resource Languages}
%source: https://aclanthology.org/2022.emnlp-main.102.pdf
While high-resource languages like English no longer use POS tagging in some popular NLP applications like AI assistants by making use of the massive collections of curated data now available, most languages do not have the luxury of large, high-quality datasets. In these low-resource languages, POS tags are still critical for NLP tasks.
%benefit to translation in low-resource languages
%source: https://www.sciencedirect.com/science/article/pii/S2405844022016632
For a task like machine translation, (<ref>) found that using linguistic features derived from POS tags improved the translation performance not only between the low-resource language pair of Thai and Myanmar but also between Thai and English and Myanmar and English. While the highest accuracy was found on English-Thai translations, likely due to both languages having a Subject-Verb-Object sentence structure, the English-Myanmar and Thai-Myanmar pairs both saw improvements over the baseline.

%why finding the best model for a high-resource language helps in low-resource languages
%source: https://aclanthology.org/2022.emnlp-main.102.pdf
While some low-resource languages have POS taggers or have the resources available for one to be reasonably made, many such languages do not. In such cases, POS taggers for high-resource languages are still quite useful. In low-resource languages where training data is scarce, one established method of finding training data is to use cross-linguistic transfer to automatically generate POS training data. This is done by lining up texts that appear in both a high-resource language and the target low-resource language and running POS taggers and parsers for the high-resource language (<ref>).
The results from these taggers and parsers are then projected onto the low-resource text. With an advanced projection method such as Graph Label Propagation (GLP) proposed by (<ref>) to transfer tags from only English, tagging accuracy on a variety of languages averaged 80.6\%. Notably, methods such as GLP can leverage multiple parallel texts from high-resource languages to increase performance even further, with tests on the same variety of languages achieving an accuracy of 84.0\%. However, leveraging multiple languages simultaneously is not always possible for certain low-resource languages due to the lack of available parallel data.

%source: https://arxiv.org/abs/2505.13908
As well as being useful, the efficacy of cross-linguistic transfer is predictable. Reference (<ref>) found that language families and morphological structures have a major impact on the performance of cross-linguistic performance. Specifically, performance was much higher when the source model for cross-linguistic transfer was in the same language family as the target language. This was especially true of morphologically-rich languages.
Even outside of the same family, cross-linguistic transfer had better performance when moving between morphologically-rich languages, contrasting with morphologically-poor languages experiencing steeper drops in performance in similar cross-family settings.
Importantly, no data from the target language was used for training or fine-tuning in these tests, which indicates that a target language being low-resource does not impact the efficacy of cross-linguistic transfer.

%contextual embeddings help
%source: https://aclanthology.org/2020.emnlp-main.391.pdf
pass

\subsection{Benefits of an English POS tagger}
%source: https://aclanthology.org/2022.acl-long.108.pdf
Finding a good POS tagger for English, a high-resource language, has clear benefits for linguistically-related low-resource languages due to the success of cross-linguistic transfer. One example is Scots, a language in the Anglic family currently considered ``vulnerable'' by the UNESCO Atlas of the World's Languages in Danger (<ref>). Research suggests that NLP can support and help revitalise endangered languages (<ref>) through machine translation, TTS, and integration into learning materials.
%source: https://blog.duolingo.com/large-language-model-duolingo-lessons/
With competent machine translation, more materials can be translated into Scots, which encourages learning and expands which facets of life can be interacted with in Scots. TTS has applications when paired with translation and learning tools, and brings one facet of accessibility into the language as well. Finally, machine-based language learning apps like Duolingo have made use of NLP and AI to provide more language learning content (<ref>), and members of the Scots speaker community can make use of POS tagging directly to help with learning Scots linguistics.

To this end, we will test multiple prominent word embedding models on the same English dataset in order to determine the best model. Knowing the best model <--FINISH THIS-->
%why embeddings instead of LLMs
%source: https://www.researchgate.net/profile/Kalpita-Birhade/publication/398054170_POS_tagging_with_Neural_networks_vs_Large_language_models_LLMs/links/6928a265abe27c41e5161fe5/POS-tagging-with-Neural-networks-vs-Large-language-models-LLMs.pdf
%For this study, word embeddings have been chosen over the recently-popular GPT-style architectures for their size, ability to be fine-tuned for very high accuracy, and customisation, while still being able to provide accurate tags (<ref>).
%Lightweight neural models that use embeddings such as BiLSTM-CRF often take less than 50MB of space, and BERT-based models often take between 300-600MB of space, making it feasible for them to be deployed and run directly on-location. On the other hand, GPT models are typically very large and very computationally demanding, meaning they often cannot be run on location and must be cloud-based. In addition to the space-efficiency being useful for in-the-field or near-the-field research, this also makes them accessible to language communities that have less powerful hardware.
%While embedding-based models need to be trained or fine-tuned to produce good results, after fine-tuning they perform better than GPT models, with BERT-based models achieving above 97\% tagging accuracy, and lighter models providing competitive tagging accuracy (<ref>). While GPT models do not need to be trained, they have a lower accuracy on tagging, around 90-93\%.
%The ability to train and fine-tune embedding-based models means that they can be directly customised to work with new data, domains, languages, and styles, which is very useful in academic settings or in settings that need specific solutions. While prompt engineering can get GPT models to work better for specific tasks, the lack of training ability and limited fine-tuning capability means it lacks the fine-grained control often needed for task-specific applications (<ref>).

%meaning disambiguation and understanding
%decoder-only LLMs do worse at meaning understanding https://aclanthology.org/2024.findings-acl.967/

\section{Related Work}
\subsection{Existing Reviews of POS Taggers}
%papers that do meta-studies but more general
<--INTRO SENTENCE-->. References (<ref>) and (<ref>) conduct literature reviews of many POS tagging papers published between 2017 and 2023, looking at a variety of deep learning and machine learning approaches. The studies chosen for these reviews span a wide range of both high-resource and low-resource languages, and cover a wide range of topics, from sentiment analysis, subject-predicate recognition, and POS tagging in unusual environments such as social media posts.

A common issue with most of the studies is the lack of a standardised dataset for training or testing on. As a result, the findings from the studies reviewed cannot be directly compared, even between papers analysing the same language. This serves as part of the motivation for our work here. Testing various word embeddings on a standard dataset enables direct comparisons of the embeddings used, as relative performance between NLP implementations is important for the development of the field.

Although many languages were represented in the studies selected for review, there was very little representation of the English language among the selection. Contemporary comparisons of POS taggers on English focus more on specific applications and more niche datasets. This serves as another part of the motivation for our work here. <--FINISH THIS-->

\subsection{Studies on Cross-Linguistic Transfer}
%paper about cross-linguistic transfer but it only used 1 BERT variant
Discussed above in \ref{sec_motivations}, (<ref>) provides crucial insights into the efficacy and patterns of cross-linguistic transfer. In this study, source-target language pairs were created from the chosen languages, and the XLM-RoBERTa multilingual transformer model was fine-tuned on each source language to serve as the POS tagger. Each pair was then analysed in terms of model performance, both quantitatively through precision, recall, and F1 scores, and qualitatively, through examining the areas where errors occurred to find commonly-appearing linguistic features such as morphology.
However, this study only uses BERT embeddings. <--FINISH THIS-->

%paper on graph label projection
Like (<ref.prev>), (<ref>) uses word embeddings generated by XLM-RoBERTa, <>
%once GLP generates data, trains a BiLSTM tagger with XLM-RoBERTa embeddings then tests that.
%used Stanza to tag in high-resource language

\subsection{Word Embeddings Used}
%the embeddings we're working with: FLAIR, XLNet, BERT, T5, FastText
%an embedding that needs mentioning: Word2Vec
%for each embedding: background information from the paper it's introduced in, evidence of someone else using it
%FastText was used as the static baseline because it can handle OOV words
%we need to say something about BERT using contextual word embeddings

\section{Methods}
\subsection{Hypotheses}
We hypothesize that high-quality embeddings trained on a high-resource language can result in good POS tagging performance in a related low-resource language, and that high-quality embeddings for a high-resource language will be high-quality embeddings in low-resource languages. We also hypothesize that the powerful transformer based embeddings (citations) will result in superior performance on such tasks over more classical embedding methods (citations). Essentially, we will find the best embedding model for use in POS tagging of low-resource languages that are related to English.

\subsection{Datasets}
In order to meaningfully compare embeddings head to head, the classifiers using them to tag sentences must be trained and tested on the same datasets.

Each of the embedding models evaluated in this paper are trained on massive amounts of English data, so we initially test them on the Universal Dependencies English Web Treebank dataset (UD\_ENGLISH) (citation), which contains sentences sourced from various forms of media. The Universal Dependencies English Web Treebank contains 16622 sentences that are split into three partitions: train, dev, and test, as illustrated in Table \ref{table_data_split}.

In order to test the models' effectiveness at embedding text from low resource languages that are related to English, we use the Norwegian Dependency Treebank - BokmÃ¥l (UD\_NORWEGIAN) (citation). However, Norwegian is not a low-resource language. Therefore we downsample the dataset to just 5\% of it's original size, resulting in a corpus containing 1002 sentences partitioned as illustrated in Table \ref{table_data_split}. This simulates a scenario where the data available for the target language is scarce, similar to how it is for real low-resource languages.

\begin{table}[htbp]
\renewcommand{\arraystretch}{1.3} 
\caption{Distribution of Sentence Data}
\label{table_data_split}
\centering
\begin{tabular}{lcccc}
\toprule
& \textbf{English} & \textbf{Norwegian}\\
\midrule
\textbf{Train} & 12544 & 785 \\ 
\textbf{Dev} & 2001 & 120 \\ 
\textbf{Test} & 2077 & 97 \\
\bottomrule
\end{tabular}
\end{table}

The Universal Dependencies Treebank datasets are used for multiple reasons, including their diverse data sources and manageable size for local hardware. However, their most important features are their use of the Universal Part of Speech tagset (UPOS), designed to be a universal POS tag standard for multilingual applications, and that the annotations are hand corrected by humans to ensure 100\% label accuracy.
% One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling
% Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books IEEE
% Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

\subsection{Evaluation Metrics}
To evaluate the taggers, we calculate macro precision, macro recall, and micro F1 score. Macro precision is the average precision of each of the \(N\) classes, weighted equally:
\[\text{Macro Precision} = \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}\displaystyle\frac{TP_i}{TP_i + FP_i}\]
Macro recall is the average recall of each class weighted equally:
\[\text{Macro Recall} = \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}\displaystyle\frac{TP_i}{TP_i + FN_i}\]
Macro precision and recall measure a model's performance on rare classes, uncommonly-seen syntactic roles such as interjections (UPOS tag \texttt{INTJ}). Micro F1 score is actually equivalent to the average accuracy in multi-class classification tasks:
\[\text{Micro F1} = \displaystyle\frac{2\sum_{i=1}^{N}TP_i}{2(\sum_{i=1}^{N}TP_i + \sum_{i=1}^{N}FP_i + \sum_{i=1}^{N}FN_i)}\]
Micro F1 score is used as a measure of overall performance.

\subsection{Classifier Model Architecture}
In order to isolate the embeddings in our tests, we use the same architecture to classify the embeddings from each embedding model. It is also critical that the performance of each model can be attributed to the embeddings rather than to the classifier itself, so a simple linear classifier is used. In each case, the linear classifier takes as input the embeddings and outputs a vector of size 17\textemdash one feature for each tag in the UPOS standard. Locked dropout with probability 0.5 is applied to the embeddings before classification.

\subsection{Model Training}
Each model is trained using the Adam optimizer (citation) and cross-entropy loss. The learning rate is tailored to each model, and adjusted automatically with a learning rate scheduler to ensure fast convergence without instability or over-fitting. For the transformer (citation) based embedding models, the final layer of the embedding model itself is fine-tuned alongside the linear classifier.

\subsubsection{Framework}
(flair citation) released a comprehensive NLP framework that leverages PyTorch (citation) alongside their contextual string embeddings that are evaluated in this paper. The Flair framework is used to implement, train, and evaluate each of the embedding models. All training and evaluation was executed locally using CUDA on an NVIDIA RTX4060-mobile GPU.

\subsubsection{Training on English Data}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{code/training_summary.pdf}
	\caption{Training vs. development loss and development accuracy per epoch on English data.}
	\label{figure_english_training}
\end{figure}

As a baseline for measuring the performance of each embedding model, we train over the English dataset. Each model is trained for 10 epochs with min-batch size of 16. Longer training stints could be considered, but given the relatively small quantity of data, too much could result in over-fitting. Moreover, as illustrated in Fig. \ref{figure_english_training}, each model converges rather quickly (after only 4-6 epochs). This demonstrates that additional training would have diminishing returns.
Figure \ref{figure_english_training} also indicates that the baseline static embedding model\textemdash FastText, struggles to converge. This is likely due to its inability to capture contextual information.

\subsubsection{Training on Norwegian Data}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{code/norwegian_training_summary.pdf}
	\caption{Training vs. development loss and development accuracy per epoch on Norwegian data.}
	\label{figure_norwegian_training}
\end{figure}
Next, to compare performance in the high-resource language to the performance in the target language, each model is freshly trained on the downsampled Norwegian dataset, starting from the same initial state as the English models. Again, the models are trained for 10 epochs with min-batch size of 16, to avoid over-fitting. Figure 
\ref{figure_norwegian_training} 
shows that each model converges nicely (apart from the static embedding model), as they do when trained on English data. Unsurprisingly, the development loss and accuracy do not reach the same levels as in Fig.
\ref{figure_english_training}.

\subsubsection{Fine-Tuning English Models on Norwegian Data}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{code/transfer_training_summary.pdf}
	\caption{Training vs. development loss and development accuracy per epoch during the fine-tuning of English models on Norwegian data.}
	\label{figure_transfer_training}
\end{figure}
Finally, while most of the heavy lifting of the models comes from the embeddings, we are interested to see if pre-training the simple classifier head on English data can give another performance boost. Therefore, we fine-tune the trained English taggers on the Norwegian dataset for an additional 5 epochs with a min-batch size of 16. The convergence of loss and accuracy can be seen in Figure \ref{figure_transfer_training}.

\section{Results}
\subsection{English Model Performance}
Rather unsurprisingly, the FastText embeddings yielded the worst performance, followed by the Flair embeddings, and then the three transformer based models performed similarly, each achieving an F1 score \(> 97\%\). The macro precision, macro recall, and micro F1 score are shown for each model in Table 
\ref{table_model_results}.
The transformer models also perform quite well on the rare classes, as indicated by high macro precision and recall.

\begin{table}[htbp]
\renewcommand{\arraystretch}{1.3} 
\caption{Performance of Taggers Trained on English}
\label{table_model_results}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Embeddings} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
FastText & 0.7944 & 0.7506 & 0.8471 \\
Flair & 0.8698 & 0.8079 & 0.9300 \\ 
BERT & \textbf{0.9499} & 0.9163 & 0.9714 \\
XLNet & 0.9486 & \textbf{0.9177} & 0.9745 \\ 
T5 & 0.9116 & 0.9152 & \textbf{0.9764} \\ 
\bottomrule
\end{tabular}
\end{table}

\subsection{Norwegian Model Performance}
Table \ref{table_model_results_norwegian} shows the results of training the taggers on Norwegian directly. Despite never being trained on Norwegian text apart from the limited training done in this study, both XLNet and T5 performed quite well on the Norwegian dataset. T5 was the most robust of the models, losing only approximately 0.06 on its F1 score. This indicates a strong ability to generalize to similar languages. All models also saw large drops in macro precision and recall, which can be attributed to the lack of rare class representation in a smaller dataset.

\begin{table}[htbp]
\renewcommand{\arraystretch}{1.3}
\caption{Performance of Taggers Trained on Norwegian}
\label{table_model_results_norwegian}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Embeddings} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
FastText & 0.7104 & 0.6533 & 0.7162 \\
Flair & 0.8796 & 0.8678 & 0.8600 \\ 
BERT & 0.8408 & 0.8354 & 0.8894 \\
XLNet & \textbf{0.8440} & \textbf{0.8507} & 0.9070 \\ 
T5 & 0.7565 & 0.7606 & \textbf{0.9163} \\ 
\bottomrule
\end{tabular}
\end{table}

\subsection{Fine-Tuned English Model Performance}
Finally, Table \ref{table_model_results_transfered} illustrates the results of fine-tuning the English taggers on the Norwegian dataset. Interestingly, despite its simplicity, pre-training the classifier head does appear to have a small performance benefit. Both T5 and XLNet see increases to their F1 score, 0.49\% and 1.22\% respectively. FastText also saw a large boost in performance of 3.59\%. 

\begin{table}[htbp]
\renewcommand{\arraystretch}{1.3} 
\caption{Performance of Taggers Fine-Tuned on Norwegian}
\label{table_model_results_transfered}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Embeddings} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
FastText & 0.8226 & 0.7021 & 0.7521 \\
Flair & 0.7755 & 0.7168 & 0.8264 \\ 
BERT & 0.8364 & 0.8481 & 0.8815 \\
XLNet & 0.7614 & 0.7652 & 0.9192 \\ 
T5 & \textbf{0.8710} & \textbf{0.8629} & \textbf{0.9212} \\ 
\bottomrule
\end{tabular}
\end{table}

\section{Results and Discussion}
\subsection{Embedding Quality}
In our first experiment, we found that the transformer embeddings BERT, XLNet, and T5 were of the highest quality. When tested on an even playing field (The same classifier, training procedures, etc.), they significantly outperformed the non-transformer contextual embedding, which itself significantly outperformed the static embedding baseline of FastText. This indicates that their contextual embeddings, especially transformer-based contextual embeddings, encode a deeper understanding of the underlying syntactic and semantic patterns of the English language, and are thus better suited to POS tagging tasks.

\subsection{Generalizability}
In our second experiment, we found that higher quality embeddings, and therefore higher POS tagging performance, on a high-resource language results in higher performance on a related low-resource language. The models using T5, XLNet, and BERT, who achieved very high accuracy on the English dataset, performed similarly relative to each other and the other models on the downsampled Norwegian dataset. This indicates that the transformer embeddings are capable of generalizing to languages that are related to English, including low-resource languages.

This easy generalizability is notable in itself in, as it suggests that an embedding model that performs better than its competition in a high-resource language is likely to perform better than that same competition in a lower-resource language. This is especially useful for studies on other languages, as many studies that used word embeddings for work in other languages used static word embeddings, often Word2Vec (<ref.metastudies>).

\subsection{Pre-Training and Fine-Tuning}
While not the focus of this paper, our third experiment revealed that pre-training the classifier head of the tagging model on the source language before fine-tuning the whole model on a target language can result in a performance increase even with the simplest possible classifier. It would not be crazy to assume that the benefits of such pre-training would be amplified by a more sophisticated classification model like many of the state-of-the-art taggers, who use a BiLSTM.

This performance increase for a related low-resource language indicates that linguistic similarities can be taken advantage of for cross-linguistic transfer even without access to a parallel corpus, which is very encouraging for work on low-resource languages that lack such corpora.

\subsection{Which Embeddings Should Be Used?}
It is clear that static embeddings should not be used for POS tagging in low-resource languages related to English, or really any POS tagger in general. Unless there are significant time or compute restrictions in place preventing the use of larger models like T5, static word embeddings are overshadowed. Even so, many large models like T5, XLNet, and BERT have been released with alternatives with fewer parameters for computationally-restricted environments.

For POS tagging in English, any of the transformer-based models can provide extremely rich embeddings capable of very high performance when partnered with a sufficient classifier. Even with the simplest possible classifier, the transformer embeddings reach near perfect performance (Table \ref{table_model_results}). For POS tagging in other languages, transformer-based contextual embeddings should be prioritised over static embeddings where possible.

While T5, XLNet, and BERT demonstrated very similar performance when tested on English, BERT fell behind when evaluated on the simulated low-resource related language. Between T5 and XLNet, T5 was marginally more capable of generalizing to the new language.

\section{Conclusion}
In this paper, we study the POS tagging performance of popular word embeddings for English, as well as briefly examine the theoretical impacts of high-quality word embeddings in English on POS tagging and NLP in other languages.
%hypotheses: transformer embeddings better, good embeddings for high-resource are good for low-resource, good embeddings for high-resource tag well on low-resource

We found that that the transformer-based contextual word embeddings of BERT, XLNet, and T5 performed best with F1 scores all over 97\%, and that contextual word embeddings in general greatly outperformed static word embeddings. This is consistent with linguistic theory, and suggests that properly accounting for and encoding context is important for many NLP tasks even beyond POS tagging.

Regarding the transfer of models between languages, we found that relative performance between embedding models stayed consistent when training embeddings and models on a related language, and furthermore, that this holds for a low-resource related language. This is promising for new work in these languages, and we encourage the use of transformer-based contextual embedding models on low-resource languages wherever possible. <>

Somewhat consistent with existing literature cross-linguistic transfer of POS tags into low-resource languages, we found that training models and embeddings on English before fine-tuning the models on Norwegian raised F1 scores for the best models, T5 and XLNet, when compared to their training results on Norwegian. Interestingly, BERT's performance on this task actually decreased compared to its Norwegian results. While the studies we found on cross-linguistic transfer used RoBERTa (<ref>),(<ref>), a variant of BERT compared to our use of base BERT, it is worth considering the use of T5 and XLNet in future studies on this topic.

\subsection{Future Work}
%future work on comparing taggers
%future work on low-resource languages

\section{Acknowledgements}
We did not use any LLMs to assist in writing this report.

%\section*{References}
\begin{thebibliography}{00}
\bibitem{b1} Pass
\end{thebibliography}

\end{document}
