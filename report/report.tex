\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{linguex}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Part-of-Speech Tagging using Contextual Word Embeddings}

\author{\IEEEauthorblockN{Juhani Dickinson}
\IEEEauthorblockA{\texttt{jdickin9@uwo.ca}}
\and
\IEEEauthorblockN{Paul Moore}
\IEEEauthorblockA{\texttt{email address or ORCID}}
}

\maketitle

\begin{abstract}
ABSTRACT
\end{abstract}

\begin{IEEEkeywords}
pos tagging, part of speech, nlp, word embedding, contextual word embedding
\end{IEEEkeywords}

\section{Introduction}
\subsection{Parts of Speech}
In a sentence, each word has a syntactic role to play. In the sentence ``The brown dog runs quickly towards food.'', ``dog'' denotes a thing, ``brown'' modifies or describes ``dog'', ``runs'' denotes an action, and so on. These syntactic roles are commonly referred to as ``parts of speech'', and are mapped to words using part-of-speech tags.
However, this is not a one-to-one mapping. Many words have multiple meanings, and in many cases, these different meanings play different syntactic roles, mapping them to different parts of speech. For example, the word ``orange'', spelled and pronounced identically, can be either a noun or an adjective, as seen in sentences (\ref{orange_n}) and (\ref{orange_adj}).
\begin{align}
	\text{``I often eat an }&\text{\underline{orange} after working out.'' (Noun)}\label{orange_n}\\
	\text{``The }&\text{\underline{orange} car has arrived.'' (Adjective)}\label{orange_adj}
\end{align}

Figuring out what part of speech a word has is not an unsolvable task, however. Where a word is in the sentence and what words surround it play a major role in disambiguating between syntactic roles.
In (\ref{orange_n}), the word after ``orange'' is ``after'', which is an adverb. ``Orange'' is either an adjective or a noun, and since the following word is an adverb, it cannot be an adjective. In (\ref{orange_adj}), the words surrounding ``orange'' are ``the'' and ``car'', which are a determiner and a noun respectively. In this position, ``orange'' describes ``car'', making it an adjective.

\subsection{Part-of-Speech (PoS) Tagging}
The task of PoS tagging (often simply called tagging) is as follows: given a sentence, label each word in the sentence with the PoS tag that describes its syntactic role, seen below for sentence \ref{pos_tag_example}.
\addtocounter{ExNo}{\value{equation}}\stepcounter{equation} %matching the linguistic example counter to align counter
%This does need 2 blank lines under it to start a new paragraph
\exg.\label{pos_tag_example} Can you \underline{spot} the large \underline{spot} on your nose ?\\
	{} AUX D VB D ADJ N PP D N .\\


What makes the task of tagging challenging is the contextual requirement: without it, both instances of ``spot'' in sentence \ref{pos_tag_example} would be tagged as either VB or N. Because of this, non-naive tagger implementations leverage some level of context to perform their task. Simple implementations like $n$-gram taggers assign tags to words based on both the word itself and the tags of the preceding $n$ words.
%something about n-gram taggers?
\subsection{Word Embeddings}
A common issue with simple tagging implementations is resistance to parallelisation. Any operation below the level of sentence must be linear, as the tag of the current word depends on the tag(s) of the previous words. To get around the linear-time requirement, some taggers use word embeddings, which represent words as vectors. This embedding takes into account the surrounding words, so that the meaning and context of each word is stored in the embedding. Using embeddings, tagging can be parallelised at the level of individual words.
\subsubsection{Static Word Embeddings}
pass
\subsubsection{Contextual Word Embeddings}
pass

\section{Related Work}
pass
%papers that do meta-studies but more general
%the embeddings we're working with: FLAIR, BERT, T5, GloVe, FLAIR BiLSTM

\section{Motivation}
%PoS tagging benefit to NLP: TTS, word-sense disambiguation, word order, <--more?-->
%source: http://103.203.175.90:81/fdScript/RootOfEBooks/E%20Book%20collection%20-%202024%20-%20D/CSE%20%20IT%20AIDS%20ML/NATURAL%20LANGUAGE%20PROCESSING%20IN%20ARTIFICIAL%20INTELLIGENCE.pdf#page=150
Part-of-speech tags are useful and sometimes key pieces of information in many natural language processing (NLP) tasks such as text-to-speech (TTS), word-sense disambiguation, marking word order, and named entity recognition (<ref>).
In TTS applications, the pronunciation of a word can change depending on what its syntactic role is or sometimes even what tense it is. The word ``resume'' is pronounced differently based on whether it is meant as ``continue doing'' or as ``a document detailing professional experience'', which can be easily disambiguated based on whether it acts as a verb or a noun.
This disambiguation-by-tag is also used in word-sense disambiguation, like in sentence \ref{pos_tag_example}. In a search-engine setting, understanding whether ``spot'' refers to an activity or a visual marker allows for more accurate search results.

%benefit to nlp in low-resource languages
%source: https://aclanthology.org/2022.emnlp-main.102.pdf
While high-resource languages like English no longer use PoS tagging in some popular NLP applications like AI assistants by making use of the massive collections of curated data now available, most languages do not have the luxury of large, high-quality datasets. In these low-resource languages, PoS tags are still critical for NLP tasks.

%benefit to translation in low-resource languages
%source: https://www.sciencedirect.com/science/article/pii/S2405844022016632
For a task like machine translation, (<ref>) found that using linguistic features derived from PoS tags improved the translation performance not only between the low-resource language pair of Thai and Myanmar but also between Thai and English and Myanmar and English. <--MORE-->

%why finding the best model for a high-resource language helps in low-resource languages
%sources: https://arxiv.org/abs/2505.13908 :: https://aclanthology.org/2022.emnlp-main.102.pdf
<--INTRO SENTENCE-->. (<ref>) <>. %graph propagation on turkish
As well as being useful, the efficacy of cross-linguistic transfer is predictable. (<ref>) found that language families and morphological structures have a major impact on the performance of cross-linguistic performance. <--TALK ABOUT RESULTS-->.

Finding the best PoS tagger for English, a high-resource language, has clear benefits for related low-resource languages. One example is Scots, a language in the Anglic family currently considered ``vulnerable'' by the UNESCO Atlas of the World's Languages in Danger (<ref>). Research suggests that NLP can support and help revitalise endangered languages (<ref>) through machine translation, TTS, and integration into learning materials. <--MORE-->
%source: https://aclanthology.org/2022.acl-long.108.pdf

%meaning disambiguation and understanding
%decoder-only LLMs do worse at meaning understanding https://aclanthology.org/2024.findings-acl.967/

\section{Methods}
pass

\section{Experimental Results}
pass

\section{Conclusions}
pass

%\section*{References}
\begin{thebibliography}{00}
\bibitem{b1} Pass
\end{thebibliography}

\end{document}
