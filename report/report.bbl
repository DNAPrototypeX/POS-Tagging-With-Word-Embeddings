% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{ghosh2020parts}
S.~Ghosh and B.~K. Mishra, ``Parts-of-speech tagging in nlp: Utility, types,
  and some popular pos taggers,'' in \emph{Natural Language Processing in
  Artificial Intelligence}.\hskip 1em plus 0.5em minus 0.4em\relax Apple
  Academic Press, 2020, pp. 131--165.

\bibitem{imanigooghari-etal-2022-graph}
\BIBentryALTinterwordspacing
A.~Imani, S.~Severini, M.~Jalili~Sabet, F.~Yvon, and H.~Sch{\"u}tze,
  ``Graph-based multilingual label propagation for low-resource part-of-speech
  tagging,'' in \emph{Proceedings of the 2022 Conference on Empirical Methods
  in Natural Language Processing}, Y.~Goldberg, Z.~Kozareva, and Y.~Zhang,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Abu Dhabi, United Arab Emirates:
  Association for Computational Linguistics, Dec. 2022, pp. 1577--1589.
  [Online]. Available: \url{https://aclanthology.org/2022.emnlp-main.102/}
\BIBentrySTDinterwordspacing

\bibitem{hlaing2022improving}
Z.~Z. Hlaing, Y.~K. Thu, T.~Supnithi, and P.~Netisopakul, ``Improving neural
  machine translation with pos-tag features for low-resource language pairs,''
  \emph{Heliyon}, vol.~8, no.~8, 2022.

\bibitem{crosslinguistictransfer}
\BIBentryALTinterwordspacing
A.~Bankula and P.~Bankula, ``Cross-linguistic transfer in multilingual nlp: The
  role of language families and morphology,'' 2025. [Online]. Available:
  \url{https://arxiv.org/abs/2505.13908}
\BIBentrySTDinterwordspacing

\bibitem{moseley2010atlas}
C.~Moseley, \emph{Atlas of the World's Languages in Danger}.\hskip 1em plus
  0.5em minus 0.4em\relax Unesco, 2010.

\bibitem{zhang-etal-2022-nlp}
\BIBentryALTinterwordspacing
S.~Zhang, B.~Frey, and M.~Bansal, ``How can {NLP} help revitalize endangered
  languages? a case study and roadmap for the {C}herokee language,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, S.~Muresan, P.~Nakov, and
  A.~Villavicencio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Dublin,
  Ireland: Association for Computational Linguistics, May 2022, pp. 1529--1541.
  [Online]. Available: \url{https://aclanthology.org/2022.acl-long.108/}
\BIBentrySTDinterwordspacing

\bibitem{duolingo}
``How duolingo uses ai to create lessons faster,''
  \url{https://blog.duolingo.com/large-language-model-duolingo-lessons/},
  accessed: 2025-12-18.

\bibitem{chiche2022part}
A.~Chiche and B.~Yitagesu, ``Part of speech tagging: a systematic review of
  deep learning and machine learning approaches,'' \emph{Journal of Big Data},
  vol.~9, no.~1, p.~10, 2022.

\bibitem{baruah2025comparative}
N.~Baruah and P.~J. Goutom, ``A comparative analysis of deep learning and
  machine learning for pos tagging,'' \emph{Expert Systems with Applications},
  vol. 288, p. 128026, 2025.

\bibitem{fasttext}
A.~Joulin, E.~Grave, P.~Bojanowski, and T.~Mikolov, ``Bag of tricks for efficient text classification,'' in \emph{Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers}, M.~Lapata, P.~Blunsom, and A.~Koller, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Valencia, Spain: Association for Computational Linguistics, Apr. 2017.

\bibitem{badri2022combining}
N.~Badri, F.~Kboubi, and A.~H. Chaibi, ``Combining fasttext and glove word embedding for offensive and hate speech text detection,'' \emph{Procedia Computer Science}, vol. 207, pp. 769--778, 2022.

\bibitem{T5}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a unified text-to-text transformer,'' \emph{Journal of Machine Learning Research}, vol.~21, no. 140, pp. 1--67, 2020.

\bibitem{flair}
A.~Akbik, D.~Blythe, and R.~Vollgraf, ``Contextual string embeddings for sequence labeling,'' in \emph{Proceedings of the 27th International Conference on Computational Linguistics}, E.~M. Bender, L.~Derczynski, and P.~Isabelle, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Santa Fe, New Mexico, USA: Association for Computational Linguistics, Aug. 2018, pp. 1638--1649.

\bibitem{rojas2022clinical}
M.~Rojas, J.~Dunstan, and F.~Villena, ``Clinical flair: A pre-trained language model for spanish clinical natural language processing,'' in \emph{Proceedings of the 4th Clinical Natural Language Processing Workshop}, 2022, pp. 87--92.

\bibitem{devlin-etal-2019-bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``{BERT}: Pre-training of deep bidirectional transformers for language understanding,'' in \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, J.~Burstein, C.~Doran, and T.~Solorio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171--4186.

\bibitem{wang2019text}
Q.~Wang, P.~Liu, Z.~Zhu, H.~Yin, Q.~Zhang, and L.~Zhang, ``A text abstraction summary model based on bert word embedding and reinforcement learning,'' \emph{Applied Sciences}, vol.~9, no.~21, p. 4701, 2019.

\bibitem{XLNet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~R. Salakhutdinov, and Q.~V. Le, ``Xlnet: Generalized autoregressive pretraining for language understanding,'' in \emph{Advances in Neural Information Processing Systems}, H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, Eds., vol.~32.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2019.

\bibitem{vaswani_attention_2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~u. Kaiser, and I.~Polosukhin, ``Attention is {All} you {Need},'' in \emph{Advances in {Neural} {Information} {Processing} {Systems}}, I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, Eds., vol.~30.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2017.

\bibitem{word2vec}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word representations in vector space,'' \emph{Proceedings of Workshop at ICLR}, vol. 2013, 01 2013.

\bibitem{UD_ENGLISH}
N.~Silveira, T.~Dozat, M.-C. de~Marneffe, S.~Bowman, M.~Connor, J.~Bauer, and C.~D. Manning, ``A gold standard dependency corpus for {E}nglish,'' in \emph{Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014)}, 2014.

\bibitem{UD_NORWEGIAN}
L.~{\O}vrelid and P.~Hohle, ``Universal dependencies for norwegian,'' in \emph{International Conference on Language Resources and Evaluation}, 2016.

\bibitem{UD}
M.-C. de~Marneffe, C.~D. Manning, J.~Nivre, and D.~Zeman, ``Universal dependencies,'' \emph{Computational Linguistics}, vol.~47, no.~2, pp. 255--308, 07 2021.

\bibitem{FLERT}
S.~Schweter and A.~Akbik, ``Flert: Document-level features for named entity recognition,'' \emph{ArXiv}, vol. abs/2011.06993, 2020.

\bibitem{Kingma2014AdamAM}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' \emph{CoRR}, vol. abs/1412.6980, 2014.

\bibitem{PyTorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and S.~Chintala, ``Pytorch: An imperative style, high-performance deep learning library,'' in \emph{Advances in Neural Information Processing Systems}, H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, Eds., vol.~32.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2019.

\end{thebibliography}
